---
layout: post
title:  "Learning Robust, Agile, Natural Legged Locomotion Skills in the Wild"
date:   2023-03-01 21:21:53 +00:00
image: /images/amp-in-the-wild.gif
categories: research
authors: "<strong>Yikai Wang</strong>*, Zheyuan Jiang*, Jianyu Chen"
venue: "Submitted to ICRA 2024"
website: https://sites.google.com/view/adaptive-multiskill-locomotion
arxiv: https://arxiv.org/abs/2304.10888
#code: https://github.com/wangyiji20/Fall_Recovery_control.git
video: https://youtu.be/lzZOxLW4YXg
---

We propose a new framework for learning robust, agile and natural legged locomotion skills over challenging terrain with only proprioceptive perception. We incorporate an adversarial training branch based on real animal locomotion data upon a teacher-student training pipeline for robust sim-to-real transfer. To the best of our understanding, this is the first
learning-based method enabling quadrupedal robots to gallop in the wild.

I wrote a paper describing the details of a family of RGBD cameras, ASICs and algorithms produced by Intel. It was submitted and accepted to CCD 2017, a CVPR 2017 Workshop. My coauthors were all senior management at Intel and the paper was written to inform the academic community of issues, challenges and priorities in building stereoscopic depth cameras for production use. We highlight state-of-the-art performance on modern datasets, on certain metrics, along with establishing baselines for new datasets and evaluation metrics for depth cameras in general. 

